# Boris-LLM

## Description
Boris-LLM est un assistant automatisÃ© de pentest qui utilise l'intelligence artificielle pour effectuer des tests de pÃ©nÃ©tration de maniÃ¨re autonome. L'outil s'appuie sur un modÃ¨le de langage local (via Ollama) pour analyser progressivement une cible, choisir les tests appropriÃ©s, et gÃ©nÃ©rer des rapports dÃ©taillÃ©s.

## ğŸŒŸ CaractÃ©ristiques
- Tests automatisÃ©s et adaptatifs
- Analyse progressive et intelligente
- Support des tests Web et RÃ©seau
- Rapports dÃ©taillÃ©s en Markdown
- Environnement isolÃ© via Docker
- IntÃ©gration avec Ollama (LLM local)

## ğŸ› ï¸ PrÃ©requis
- Docker et Docker Compose
- Ollama installÃ© et configurant sur la machine hÃ´te
- Au moins 8 Go de RAM disponible (ici 32 pour un fonctionnement correct)
- Le modÃ¨le llama3.1:lastest chargÃ© dans Ollama (benchmark en cours avec phi4-mini)

## ğŸ“¦ Installation

1. Cloner le dÃ©pÃ´t :
```bash
git clone https://github.com/clab60917/boris-llm.git
cd boris-llm
```

2. S'assurer qu'Ollama est en cours d'exÃ©cution avec le modÃ¨le appropriÃ© :
```bash
ollama pull llama3.1:lastest
ollama run llama3.1:lastest
```

3. Construire l'environnement Docker :
```bash
docker-compose build
```

## ğŸš€ Utilisation

1. DÃ©marrer les services :
```bash
docker-compose up -d
```

2. Lancer le pentester :
```bash
docker exec -it llm-pentest python3 main.py
```

3. Choisir le type de pentest :
   - Web (Applications Web)
   - RÃ©seau/Serveur

4. Le systÃ¨me effectuera automatiquement :
   - Reconnaissance initiale
   - Ã‰numÃ©ration approfondie
   - Tests de vulnÃ©rabilitÃ©s
   - GÃ©nÃ©ration de rapport

## ğŸ“Š Structure du Projet
```
llm-pentester/
â”œâ”€â”€ docker-compose.yml      # Configuration Docker
â”œâ”€â”€ Dockerfile             # Configuration de l'environnement
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”œâ”€â”€ main.py               # Script principal
â”œâ”€â”€ logs/                 # Logs d'exÃ©cution
â””â”€â”€ results/              # Rapports gÃ©nÃ©rÃ©s
```

## ğŸ›¡ï¸ Outils IntÃ©grÃ©s
- nmap
- nikto
- gobuster
- whatweb
- wfuzz
- hydra
- sqlmap
- dirb
- et plus encore...

## ğŸ“ Format des Rapports
Les rapports sont gÃ©nÃ©rÃ©s au format Markdown et incluent :
- Informations sur la cible
- DÃ©couvertes majeures
- DÃ©tails des tests effectuÃ©s
- RÃ©sultats des scans
- VulnÃ©rabilitÃ©s potentielles
- Recommandations

## âš™ï¸ Configuration

### Modification des Wordlists
Les wordlists par dÃ©faut sont situÃ©es dans :
```
/usr/share/wordlists/
```

### Ajout de Nouveaux Outils
1. Ajouter l'outil dans le Dockerfile
2. Ajouter la commande dans la liste `allowed_commands`
3. Mettre Ã  jour le prompt LLM

## ğŸ” SÃ©curitÃ©
- Tous les tests sont exÃ©cutÃ©s dans un conteneur Docker isolÃ©
- Les permissions sont limitÃ©es
- Les commandes sont filtrÃ©es
- Les rÃ©sultats sont stockÃ©s localement

## âš ï¸ Avertissements
- Utilisez uniquement sur des systÃ¨mes que vous Ãªtes autorisÃ© Ã  tester
- Certains tests peuvent Ãªtre intrusifs
- Les performances dÃ©pendent de la puissance de la machine hÃ´te
- Le modÃ¨le LLM peut nÃ©cessiter des ajustements

## ğŸ› RÃ©solution des ProblÃ¨mes

### Erreurs Communes
1. **Ollama non accessible** :
   ```
   Could not connect to Ollama
   ```
   â†’ VÃ©rifier qu'Ollama est en cours d'exÃ©cution sur la machine hÃ´te

2. **Erreurs de mÃ©moire** :
   â†’ Augmenter les ressources Docker allouÃ©es

3. **Commandes non trouvÃ©es** :
   â†’ VÃ©rifier l'installation des outils dans le Dockerfile

## ğŸ“ˆ AmÃ©liorations Futures
- [ ] Support de plus de types de tests
- [ ] IntÃ©gration de nouveaux outils
- [ ] AmÃ©lioration de l'analyse des rÃ©sultats
- [ ] Interface Web
- [ ] Support de cibles multiples
- [ ] Rapports personnalisables

## ğŸ¤ Contribution
Les contributions sont les bienvenues ! Pour contribuer :
1. Fork le projet
2. CrÃ©er une branche pour votre fonctionnalitÃ©
3. Commit vos changements
4. Push sur la branche
5. CrÃ©er une Pull Request

## ğŸ“„ Licence
Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ‘¥ Contact
- CrÃ©Ã© par : clab60917

## ğŸ™ Remerciements
- Ã‰quipe Ollama pour le LLM local